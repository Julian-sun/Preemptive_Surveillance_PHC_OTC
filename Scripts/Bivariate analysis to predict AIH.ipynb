{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read packedges\n",
    "\n",
    "import pandas as pd\n",
    "import pandas.testing as tm\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.discrete.discrete_model as dm\n",
    "\n",
    "from patsy import dmatrices\n",
    "import statsmodels.graphics.tsaplots as tsa\n",
    "\n",
    "\n",
    "from scipy.fft import fft, ifft, fftfreq\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "import itertools\n",
    "from itertools import combinations, chain\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import re\n",
    "\n",
    "import functions\n",
    "\n",
    "import early_warning_detection_functions\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import pymannkendall as mk\n",
    "\n",
    "import math\n",
    "\n",
    "import trend_timeseries\n",
    "\n",
    "from scipy.stats import friedmanchisquare\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from pmdarima.preprocessing import FourierFeaturizer\n",
    "from pmdarima.datasets import load_wineind\n",
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aih = pd.read_parquet('/Users/julianeoliveira/Documents/github/Bivariate_Anomaly_Detection_Primary_Health_Care_Drug_Selling_ILI_surveillance/Results/data_manuscript_warning_aih_imed_for_regre.parquet')\n",
    "\n",
    "df = pd.read_parquet('/Users/julianeoliveira/Documents/github/Bivariate_Anomaly_Detection_Primary_Health_Care_Drug_Selling_ILI_surveillance/Results/data_manuscript_otc_phc_imed.parquet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[[ 'co_imed', 'year_week', 'year_week_ts', 'epidemi_cal_start', 'epidemi_cal_end', 'atend_ivas',\n",
    "       'num_otc_ivas', 'atend_ivas_4', 'num_otc_ivas_4', 'phc_4_lag_1', 'phc_4_lag_2',\n",
    "       'phc_4_lag_3', 'otc_4_lag_1', 'otc_4_lag_2', 'otc_4_lag_3' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aih = df_aih.merge(df2, on= ['co_imed', 'year_week'], how = 'left' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aih = data_aih.assign(phc_4_lag_0 = data_aih.atend_ivas_4,\n",
    "                   otc_4_lag_0 = data_aih.num_otc_ivas_4,\n",
    "                   aih_4_lag_0 = data_aih.aih_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run regressions - AIH and APS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cities without trend and sezonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of imediate without trend and sezonality 438 percentage 85.9\n"
     ]
    }
   ],
   "source": [
    "# select cities without trend and sezonality in PHC\n",
    "\n",
    "df1 = data_aih[(data_aih.p_value_aih_negbi_friedman >= 0.05) & (data_aih.p_values_negbi_aih_4 >= 0.05)]\n",
    "\n",
    "print('number of imediate without trend and sezonality', df1.co_imed.nunique(), 'percentage', round(df1.co_imed.nunique()*100/510,1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags_y = range(1, 4)  # Example: Using lags 1 to 4 for y_t\n",
    "lags_x = range(0, 4)  # Example: Using lags 0 to 4 for x_t\n",
    "dependent_variable = 'aih_4'  # Example dependent variable\n",
    "\n",
    "def lags_comb(variable, lags_variable):\n",
    "    \"\"\"Create cumulative lag combinations for a given variable.\"\"\"\n",
    "    lags_variable_comb = [f'{variable}_{lag}' for lag in lags_variable]\n",
    "    cumulative_sums_var = [' + '.join(lags_variable_comb[:i]) for i in range(1, len(lags_variable_comb) + 1)]\n",
    "    return cumulative_sums_var\n",
    "\n",
    "# Generate lagged combinations\n",
    "lags_y_comb = lags_comb('aih_4_lag', lags_y)\n",
    "lags_x_comb = lags_comb('phc_4_lag', lags_x)\n",
    "\n",
    "formulas = []\n",
    "\n",
    "# Iterate over all combinations and create formula strings\n",
    "for value in lags_x_comb:\n",
    "    for terms in lags_y_comb:\n",
    "        components = [terms, value]\n",
    "        # Remove empty strings and join components\n",
    "        formula_components = ' + '.join(filter(None, components))\n",
    "        formula = f'{dependent_variable} ~ {formula_components}'\n",
    "        formulas.append(formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Keep the best model formulas and AIC\n",
    "\n",
    "lst = []\n",
    "\n",
    "for code in df1.co_imed.unique():\n",
    "\n",
    "    #print(code)\n",
    "\n",
    "    data = df1[df1.co_imed == code]\n",
    "\n",
    "    # Placeholder for the best model\n",
    "    best_model = None\n",
    "    best_aic = float('inf')\n",
    "    best_formula = None\n",
    "\n",
    "    # Loop through all formulas to find the best model based on AIC\n",
    "    for formula in formulas:\n",
    "        \n",
    "        # Fit the model\n",
    "        alpha = 1\n",
    "        model = smf.glm(formula=formula, data=data[:-30], family=sm.families.NegativeBinomial(alpha=alpha)).fit()\n",
    "        \n",
    "        # Check the AIC of the current model\n",
    "        current_aic = model.aic\n",
    "        \n",
    "        # Update the best model if the current model has a lower AIC\n",
    "        if current_aic < best_aic:\n",
    "            best_aic = current_aic\n",
    "            best_model = model\n",
    "            best_formula = formula\n",
    "\n",
    "    data = data.assign(best_formula = best_formula)\n",
    "    data = data.assign(best_aic = best_aic)\n",
    "    \n",
    "    lst.append(data)\n",
    "\n",
    "lst_dfs_cities1 = lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store results\n",
    "lst = []\n",
    "model_results = []  # List to store model performance metrics\n",
    "\n",
    "for data in lst_dfs_cities1:\n",
    "    data = data.copy()  # Ensure no changes happen in place\n",
    "    \n",
    "    # Fit the model\n",
    "    alpha = 1\n",
    "    model = smf.glm(\n",
    "        formula=data.best_formula.iloc[0], \n",
    "        data=data[:-30], \n",
    "        family=sm.families.NegativeBinomial(alpha=alpha)\n",
    "    ).fit()\n",
    "    \n",
    " \n",
    "    # Assign model outputs to the DataFrame\n",
    "    data[\"forecast_aih_4\"] = None \n",
    "    data['fitted_values_aih_aps'] = None\n",
    "    data['residuals_aih_aps'] = None\n",
    "    \n",
    "    # Extract fitted values and residuals\n",
    "    data[:-30] = data[:-30].assign(\n",
    "        fitted_values_aih_aps=model.fittedvalues,\n",
    "        residuals_aih_aps=model.resid_deviance\n",
    "           )\n",
    "    \n",
    "    # Forecast\n",
    "    data[-30:] = data[-30:].assign(forecast_aih_4 = model.predict(data[-30:]))\n",
    "    data = pd.concat([data[:-30], data[-30:]], ignore_index=True)\n",
    "\n",
    "    # Extract key model statistics\n",
    "    pseudo_r2 = model.pseudo_rsquared()\n",
    "    p_values = model.pvalues\n",
    "    conf_int = model.conf_int()\n",
    "    \n",
    "    res = stats.spearmanr(data[-30:][\"aih_4\"].to_numpy(), data[-30:][\"forecast_aih_4\"].to_numpy())\n",
    "\n",
    "    # Save model statistics in a dictionary\n",
    "    model_results.append({\n",
    "        \"co_imed\": data.co_imed.iloc[0],\n",
    "        \"pseudo_R2\": pseudo_r2,\n",
    "        \"p_values\": p_values.to_dict(),\n",
    "        \"conf_int\": conf_int.values.tolist(),\n",
    "        \"log_likelihood\": model.llf,\n",
    "        \"deviance\": model.deviance,\n",
    "        \"pearson_chi2\": model.pearson_chi2,\n",
    "        'corre_forecasted': res.correlation,\n",
    "        'p_value_corr_forecast': res.pvalue\n",
    "    })\n",
    "\n",
    "    lst.append(data)  # Append modified data to the list\n",
    "\n",
    "# Update lst_dfs_cities1 with new DataFrames containing fitted values & residuals\n",
    "lst_dfs_cities1 = lst\n",
    "\n",
    "# Convert model results into a DataFrame for easy viewing\n",
    "df_model_results = pd.DataFrame(model_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'p_values' dictionary column into multiple columns\n",
    "df_pvalues = df_model_results[\"p_values\"].apply(pd.Series)\n",
    "\n",
    "# Merge expanded p-values back into the main DataFrame and drop the original dictionary column\n",
    "df_model_results1 = pd.concat([df_model_results.drop(columns=[\"p_values\"]), df_pvalues], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    438.000000\n",
       "mean       0.898024\n",
       "std        0.119186\n",
       "min       -0.522505\n",
       "25%        0.884775\n",
       "50%        0.926183\n",
       "75%        0.951764\n",
       "max        0.991546\n",
       "Name: corre_forecasted, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model_results1.corre_forecasted.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    438.000000\n",
       "mean       0.171757\n",
       "std        0.037117\n",
       "min        0.062237\n",
       "25%        0.146757\n",
       "50%        0.166820\n",
       "75%        0.191329\n",
       "max        0.351884\n",
       "Name: pseudo_R2, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model_results1.pseudo_R2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_model_results1[(df_model_results1.phc_4_lag_0 <= 0.05) | (df_model_results1.phc_4_lag_1 <= 0.05)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06621004566210045"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_model_results1[(df_model_results1.phc_4_lag_0 <= 0.05) | (df_model_results1.phc_4_lag_1 <= 0.05)])/len(df_model_results1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cities with trend and without sezonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of imediate with trend and without sezonality 31 percentage 6.1\n"
     ]
    }
   ],
   "source": [
    "df2 = data_aih[(data_aih.p_value_aih_negbi_friedman >= 0.05) & (data_aih.p_values_negbi_aih_4 < 0.05)]\n",
    "\n",
    "print('number of imediate with trend and without sezonality', df2.co_imed.nunique(), 'percentage', round(df2.co_imed.nunique()*100/510,1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags_y = range(1, 4)  # Example: Using lags 1 to 4 for y_t\n",
    "lags_x = range(0, 4)  # Example: Using lags 0 to 4 for x_t\n",
    "dependent_variable =  'aih_4' # Example dependent variable\n",
    "\n",
    "def lags_comb(variable, lags_variable):\n",
    "    \"\"\"Create cumulative lag combinations for a given variable.\"\"\"\n",
    "    lags_variable_comb = [f'{variable}_{lag}' for lag in lags_variable]\n",
    "    cumulative_sums_var = [' + '.join(lags_variable_comb[:i]) for i in range(1, len(lags_variable_comb) + 1)]\n",
    "    return cumulative_sums_var\n",
    "\n",
    "# Generate lagged combinations\n",
    "lags_y_comb = lags_comb('aih_4_lag', lags_y)\n",
    "lags_x_comb = lags_comb('phc_4_lag', lags_x)\n",
    "\n",
    "formulas = []\n",
    "\n",
    "# Iterate over all combinations and create formula strings\n",
    "for value in lags_x_comb:\n",
    "    for terms in lags_y_comb:\n",
    "        components = [terms, value]\n",
    "        # Remove empty strings and join components\n",
    "        formula_components = 'time_trend + ' + ' + '.join(filter(None, components))\n",
    "        formula = f'{dependent_variable} ~ {formula_components}'\n",
    "        formulas.append(formula)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Keep the best model formulas and AIC\n",
    "\n",
    "lst = []\n",
    "\n",
    "for code in df2.co_imed.unique():\n",
    "\n",
    "    #print(code)\n",
    "\n",
    "    data = df2[df2.co_imed == code]\n",
    "\n",
    "    # Placeholder for the best model\n",
    "    best_model = None\n",
    "    best_aic = float('inf')\n",
    "    best_formula = None\n",
    "\n",
    "    # Loop through all formulas to find the best model based on AIC\n",
    "    for formula in formulas:\n",
    "        \n",
    "        # Fit the model\n",
    "        alpha = 1\n",
    "        model = smf.glm(formula=formula, data=data[:-30], family=sm.families.NegativeBinomial(alpha=alpha)).fit()\n",
    "        \n",
    "        # Check the AIC of the current model\n",
    "        current_aic = model.aic\n",
    "        \n",
    "        # Update the best model if the current model has a lower AIC\n",
    "        if current_aic < best_aic:\n",
    "            best_aic = current_aic\n",
    "            best_model = model\n",
    "            best_formula = formula\n",
    "\n",
    "    data = data.assign(best_formula = best_formula)\n",
    "    data = data.assign(best_aic = best_aic)\n",
    "    \n",
    "    lst.append(data)\n",
    "\n",
    "lst_dfs_cities2 = lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store results\n",
    "lst = []\n",
    "model_results = []  # List to store model performance metrics\n",
    "\n",
    "for data in lst_dfs_cities2:\n",
    "    data = data.copy()  # Ensure no changes happen in place\n",
    "    \n",
    "    # Fit the model\n",
    "    alpha = 1\n",
    "    model = smf.glm(\n",
    "        formula=data.best_formula.iloc[0], \n",
    "        data=data[:-30], \n",
    "        family=sm.families.NegativeBinomial(alpha=alpha)\n",
    "    ).fit()\n",
    "    \n",
    " \n",
    "    # Assign model outputs to the DataFrame\n",
    "    data[\"forecast_aih_4\"] = None \n",
    "    data['fitted_values_aih_aps'] = None\n",
    "    data['residuals_aih_aps'] = None\n",
    "    \n",
    "    # Extract fitted values and residuals\n",
    "    data[:-30] = data[:-30].assign(\n",
    "        fitted_values_aih_aps=model.fittedvalues,\n",
    "        residuals_aih_aps=model.resid_deviance\n",
    "           )\n",
    "    \n",
    "    # Forecast\n",
    "    data[-30:] = data[-30:].assign(forecast_aih_4 = model.predict(data[-30:]))\n",
    "    data = pd.concat([data[:-30], data[-30:]], ignore_index=True)\n",
    "\n",
    "    # Extract key model statistics\n",
    "    pseudo_r2 = model.pseudo_rsquared()\n",
    "    p_values = model.pvalues\n",
    "    conf_int = model.conf_int()\n",
    "    \n",
    "    res = stats.spearmanr(data[-30:][\"aih_4\"].to_numpy(), data[-30:][\"forecast_aih_4\"].to_numpy())\n",
    "\n",
    "    # Save model statistics in a dictionary\n",
    "    model_results.append({\n",
    "        \"co_imed\": data.co_imed.iloc[0],\n",
    "        \"pseudo_R2\": pseudo_r2,\n",
    "        \"p_values\": p_values.to_dict(),\n",
    "        \"conf_int\": conf_int.values.tolist(),\n",
    "        \"log_likelihood\": model.llf,\n",
    "        \"deviance\": model.deviance,\n",
    "        \"pearson_chi2\": model.pearson_chi2,\n",
    "        'corre_forecasted': res.correlation,\n",
    "        'p_value_corr_forecast': res.pvalue\n",
    "    })\n",
    "\n",
    "    lst.append(data)  # Append modified data to the list\n",
    "\n",
    "# Update lst_dfs_cities1 with new DataFrames containing fitted values & residuals\n",
    "lst_dfs_cities2 = lst\n",
    "\n",
    "# Convert model results into a DataFrame for easy viewing\n",
    "df_model_results = pd.DataFrame(model_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'p_values' dictionary column into multiple columns\n",
    "df_pvalues = df_model_results[\"p_values\"].apply(pd.Series)\n",
    "\n",
    "# Merge expanded p-values back into the main DataFrame and drop the original dictionary column\n",
    "df_model_results2 = pd.concat([df_model_results.drop(columns=[\"p_values\"]), df_pvalues], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    31.000000\n",
       "mean      0.856747\n",
       "std       0.167419\n",
       "min       0.316902\n",
       "25%       0.858796\n",
       "50%       0.915740\n",
       "75%       0.940555\n",
       "max       0.980192\n",
       "Name: corre_forecasted, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model_results2.corre_forecasted.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    31.000000\n",
       "mean      0.195104\n",
       "std       0.055092\n",
       "min       0.129772\n",
       "25%       0.165635\n",
       "50%       0.182437\n",
       "75%       0.201881\n",
       "max       0.413385\n",
       "Name: pseudo_R2, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model_results2.pseudo_R2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_model_results2[(df_model_results2.phc_4_lag_0 <= 0.05) | (df_model_results2.phc_4_lag_1 <= 0.05)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12903225806451613"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_model_results2[(df_model_results2.phc_4_lag_0 <= 0.05) | (df_model_results2.phc_4_lag_1 <= 0.05)])/len(df_model_results2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cities without trend and with sezonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of imed without trend and with sezonality 38 percentage 7.5\n"
     ]
    }
   ],
   "source": [
    "# select cities without trend and sezonality in PHC\n",
    "\n",
    "df3 = data_aih[(data_aih.p_value_aih_negbi_friedman < 0.05) & (data_aih.p_values_negbi_aih_4 >= 0.05)]\n",
    "\n",
    "print('number of imed without trend and with sezonality', df3.co_imed.nunique(), 'percentage', round(df3.co_imed.nunique()*100/510,1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_dfs_cities3 = []\n",
    "\n",
    "for code in df3.co_imed.unique():\n",
    "    \n",
    "    set_muni = df3[df3.co_imed == code]\n",
    "\n",
    "    lst_dfs_cities3.append(set_muni)\n",
    "\n",
    "lst_dfs_cities3 = early_warning_detection_functions.harmonic(lst_dfs_cities3, 'aih_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags_y = range(1, 4)  # Example: Using lags 1 to 4 for y_t\n",
    "lags_x = range(0, 4)  # Example: Using lags 0 to 4 for x_t\n",
    "dependent_variable = 'aih_4'  # Example dependent variable\n",
    "\n",
    "def lags_comb(variable, lags_variable):\n",
    "    \"\"\"Create cumulative lag combinations for a given variable.\"\"\"\n",
    "    lags_variable_comb = [f'{variable}_{lag}' for lag in lags_variable]\n",
    "    cumulative_sums_var = [' + '.join(lags_variable_comb[:i]) for i in range(1, len(lags_variable_comb) + 1)]\n",
    "    return cumulative_sums_var\n",
    "\n",
    "# Generate lagged combinations\n",
    "lags_y_comb = lags_comb('aih_4_lag', lags_y)\n",
    "lags_x_comb = lags_comb('phc_4_lag', lags_x)\n",
    "\n",
    "formulas = []\n",
    "\n",
    "# Iterate over all combinations and create formula strings\n",
    "for value in lags_x_comb:\n",
    "    for terms in lags_y_comb:\n",
    "        components = [terms, value]\n",
    "        # Remove empty strings and join components\n",
    "        formula_components = 'Reconstructed + ' + ' + '.join(filter(None, components))\n",
    "        formula = f'{dependent_variable} ~ {formula_components}'\n",
    "        formulas.append(formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Keep the best model formulas and AIC\n",
    "\n",
    "lst = []\n",
    "\n",
    "for data in lst_dfs_cities3:\n",
    "    #print(f\"Processing {data.co_imed.iloc[0]}...\")\n",
    "\n",
    "    #print(code)\n",
    "\n",
    "    #data = df3[df3.co_imed == code]\n",
    "\n",
    "    # Placeholder for the best model\n",
    "    best_model = None\n",
    "    best_aic = float('inf')\n",
    "    best_formula = None\n",
    "\n",
    "    # Loop through all formulas to find the best model based on AIC\n",
    "    for formula in formulas:\n",
    "        \n",
    "        # Fit the model\n",
    "        alpha = 1\n",
    "        model = smf.glm(formula=formula, data=data[:-30], family=sm.families.NegativeBinomial(alpha=alpha)).fit()\n",
    "        \n",
    "        # Check the AIC of the current model\n",
    "        current_aic = model.aic\n",
    "        \n",
    "        # Update the best model if the current model has a lower AIC\n",
    "        if current_aic < best_aic:\n",
    "            best_aic = current_aic\n",
    "            best_model = model\n",
    "            best_formula = formula\n",
    "\n",
    "    data = data.assign(best_formula = best_formula)\n",
    "    data = data.assign(best_aic = best_aic)\n",
    "    \n",
    "    lst.append(data)\n",
    "\n",
    "lst_dfs_cities3 = lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store results\n",
    "lst = []\n",
    "model_results = []  # List to store model performance metrics\n",
    "\n",
    "for data in lst_dfs_cities3:\n",
    "    data = data.copy()  # Ensure no changes happen in place\n",
    "    \n",
    "    # Fit the model\n",
    "    alpha = 1\n",
    "    model = smf.glm(\n",
    "        formula=data.best_formula.iloc[0], \n",
    "        data=data[:-30], \n",
    "        family=sm.families.NegativeBinomial(alpha=alpha)\n",
    "    ).fit()\n",
    "    \n",
    " \n",
    "    # Assign model outputs to the DataFrame\n",
    "    data[\"forecast_aih_4\"] = None \n",
    "    data['fitted_values_aih_aps'] = None\n",
    "    data['residuals_aih_aps'] = None\n",
    "    \n",
    "    # Extract fitted values and residuals\n",
    "    data[:-30] = data[:-30].assign(\n",
    "        fitted_values_aih_aps=model.fittedvalues,\n",
    "        residuals_aih_aps=model.resid_deviance\n",
    "           )\n",
    "    \n",
    "    # Forecast\n",
    "    data[-30:] = data[-30:].assign(forecast_aih_4 = model.predict(data[-30:]))\n",
    "    data = pd.concat([data[:-30], data[-30:]], ignore_index=True)\n",
    "\n",
    "    # Extract key model statistics\n",
    "    pseudo_r2 = model.pseudo_rsquared()\n",
    "    p_values = model.pvalues\n",
    "    conf_int = model.conf_int()\n",
    "    \n",
    "    res = stats.spearmanr(data[-30:][\"aih_4\"].to_numpy(), data[-30:][\"forecast_aih_4\"].to_numpy())\n",
    "\n",
    "    # Save model statistics in a dictionary\n",
    "    model_results.append({\n",
    "        \"co_imed\": data.co_imed.iloc[0],\n",
    "        \"pseudo_R2\": pseudo_r2,\n",
    "        \"p_values\": p_values.to_dict(),\n",
    "        \"conf_int\": conf_int.values.tolist(),\n",
    "        \"log_likelihood\": model.llf,\n",
    "        \"deviance\": model.deviance,\n",
    "        \"pearson_chi2\": model.pearson_chi2,\n",
    "        'corre_forecasted': res.correlation,\n",
    "        'p_value_corr_forecast': res.pvalue\n",
    "    })\n",
    "\n",
    "    lst.append(data)  # Append modified data to the list\n",
    "\n",
    "# Update lst_dfs_cities1 with new DataFrames containing fitted values & residuals\n",
    "lst_dfs_cities3 = lst\n",
    "\n",
    "# Convert model results into a DataFrame for easy viewing\n",
    "df_model_results = pd.DataFrame(model_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'p_values' dictionary column into multiple columns\n",
    "df_pvalues = df_model_results[\"p_values\"].apply(pd.Series)\n",
    "\n",
    "# Merge expanded p-values back into the main DataFrame and drop the original dictionary column\n",
    "df_model_results3 = pd.concat([df_model_results.drop(columns=[\"p_values\"]), df_pvalues], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    38.000000\n",
       "mean      0.834544\n",
       "std       0.119078\n",
       "min       0.481416\n",
       "25%       0.789983\n",
       "50%       0.883360\n",
       "75%       0.927381\n",
       "max       0.970634\n",
       "Name: corre_forecasted, dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model_results3.corre_forecasted.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    38.000000\n",
       "mean      0.202300\n",
       "std       0.048317\n",
       "min       0.129779\n",
       "25%       0.168146\n",
       "50%       0.190291\n",
       "75%       0.225445\n",
       "max       0.335128\n",
       "Name: pseudo_R2, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model_results3.pseudo_R2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_model_results3[(df_model_results3.phc_4_lag_0 <= 0.05) | (df_model_results3.phc_4_lag_1 <= 0.05)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23684210526315788"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_model_results3[(df_model_results3.phc_4_lag_0 <= 0.05) | (df_model_results3.phc_4_lag_1 <= 0.05)])/len(df_model_results3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cities with trend and with sezonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of imed with trend and sezonality 3 percentage 0.6\n"
     ]
    }
   ],
   "source": [
    "# select cities without trend and sezonality in PHC\n",
    "\n",
    "df4 = data_aih[(data_aih.p_value_aih_negbi_friedman < 0.05) & (data_aih.p_values_negbi_aih_4 < 0.05)]\n",
    "\n",
    "print('number of imed with trend and sezonality', df4.co_imed.nunique(), 'percentage', round(df4.co_imed.nunique()*100/510,1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lst_dfs_cities4 = []\n",
    "\n",
    "for code in df4.co_imed.unique():\n",
    "    \n",
    "    set_muni = df4[df4.co_imed == code]\n",
    "\n",
    "    lst_dfs_cities4.append(set_muni)\n",
    "\n",
    "lst_dfs_cities4 = early_warning_detection_functions.harmonic(lst_dfs_cities4, 'aih_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags_y = range(1, 4)  # Example: Using lags 1 to 4 for y_t\n",
    "lags_x = range(0, 4)  # Example: Using lags 0 to 4 for x_t\n",
    "dependent_variable = 'aih_4'  # Example dependent variable\n",
    "\n",
    "def lags_comb(variable, lags_variable):\n",
    "    \"\"\"Create cumulative lag combinations for a given variable.\"\"\"\n",
    "    lags_variable_comb = [f'{variable}_{lag}' for lag in lags_variable]\n",
    "    cumulative_sums_var = [' + '.join(lags_variable_comb[:i]) for i in range(1, len(lags_variable_comb) + 1)]\n",
    "    return cumulative_sums_var\n",
    "\n",
    "# Generate lagged combinations\n",
    "lags_y_comb = lags_comb('aih_4_lag', lags_y)\n",
    "lags_x_comb = lags_comb('phc_4_lag', lags_x)\n",
    "\n",
    "formulas = []\n",
    "\n",
    "# Iterate over all combinations and create formula strings\n",
    "for value in lags_x_comb:\n",
    "    for terms in lags_y_comb:\n",
    "        components = [terms, value]\n",
    "        # Remove empty strings and join components\n",
    "        formula_components = 'time_trend + Reconstructed + ' + ' + '.join(filter(None, components))\n",
    "        formula = f'{dependent_variable} ~ {formula_components}'\n",
    "        formulas.append(formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 170009...\n",
      "Processing 250011...\n",
      "Processing 310054...\n"
     ]
    }
   ],
   "source": [
    "## Keep the best model formulas and AIC\n",
    "lst = []\n",
    "\n",
    "for data in lst_dfs_cities4:\n",
    "    print(f\"Processing {data.co_imed.iloc[0]}...\")\n",
    "\n",
    "    #data = df3[df3.co_ibge == code].copy()\n",
    "    data = data.assign(time_trend=np.arange(len(data)))\n",
    "\n",
    "    # Initialize placeholders\n",
    "    best_model = None\n",
    "    best_aic = float('inf')\n",
    "    best_formula = np.nan  # Set default as NaN\n",
    "\n",
    "    # Try fitting models\n",
    "    for formula in formulas:\n",
    "        try:\n",
    "            model = smf.glm(formula=formula, data=data, family=sm.families.NegativeBinomial(alpha=1)).fit()\n",
    "            current_aic = model.aic\n",
    "\n",
    "            if current_aic < best_aic:\n",
    "                best_aic = current_aic\n",
    "                best_model = model\n",
    "                best_formula = formula\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping formula {formula} for {code}: {e}\")\n",
    "            continue  # Skip to the next formula\n",
    "\n",
    "    # Assign best formula and AIC (or NaN if model failed)\n",
    "    data = data.assign(best_formula=best_formula, best_aic=best_aic if best_aic != float('inf') else np.nan)\n",
    "    lst.append(data)\n",
    "\n",
    "lst_dfs_cities4 = lst  # Store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store results\n",
    "lst = []\n",
    "model_results = []  # List to store model performance metrics\n",
    "\n",
    "for data in lst_dfs_cities4:\n",
    "    data = data.copy()  # Ensure no changes happen in place\n",
    "    \n",
    "    # Fit the model\n",
    "    alpha = 1\n",
    "    model = smf.glm(\n",
    "        formula=data.best_formula.iloc[0], \n",
    "        data=data[:-30], \n",
    "        family=sm.families.NegativeBinomial(alpha=alpha)\n",
    "    ).fit()\n",
    "    \n",
    " \n",
    "    # Assign model outputs to the DataFrame\n",
    "    data[\"forecast_aih_4\"] = None \n",
    "    data['fitted_values_aih_aps'] = None\n",
    "    data['residuals_aih_aps'] = None\n",
    "    \n",
    "    # Extract fitted values and residuals\n",
    "    data[:-30] = data[:-30].assign(\n",
    "        fitted_values_aih_aps=model.fittedvalues,\n",
    "        residuals_aih_aps=model.resid_deviance\n",
    "           )\n",
    "    \n",
    "    # Forecast\n",
    "    data[-30:] = data[-30:].assign(forecast_aih_4 = model.predict(data[-30:]))\n",
    "    data = pd.concat([data[:-30], data[-30:]], ignore_index=True)\n",
    "\n",
    "    # Extract key model statistics\n",
    "    pseudo_r2 = model.pseudo_rsquared()\n",
    "    p_values = model.pvalues\n",
    "    conf_int = model.conf_int()\n",
    "    \n",
    "    res = stats.spearmanr(data[-30:][\"aih_4\"].to_numpy(), data[-30:][\"forecast_aih_4\"].to_numpy())\n",
    "\n",
    "    # Save model statistics in a dictionary\n",
    "    model_results.append({\n",
    "        \"co_imed\": data.co_imed.iloc[0],\n",
    "        \"pseudo_R2\": pseudo_r2,\n",
    "        \"p_values\": p_values.to_dict(),\n",
    "        \"conf_int\": conf_int.values.tolist(),\n",
    "        \"log_likelihood\": model.llf,\n",
    "        \"deviance\": model.deviance,\n",
    "        \"pearson_chi2\": model.pearson_chi2,\n",
    "        'corre_forecasted': res.correlation,\n",
    "        'p_value_corr_forecast': res.pvalue\n",
    "    })\n",
    "\n",
    "    lst.append(data)  # Append modified data to the list\n",
    "\n",
    "# Update lst_dfs_cities1 with new DataFrames containing fitted values & residuals\n",
    "lst_dfs_cities4 = lst\n",
    "\n",
    "# Convert model results into a DataFrame for easy viewing\n",
    "df_model_results = pd.DataFrame(model_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'p_values' dictionary column into multiple columns\n",
    "df_pvalues = df_model_results[\"p_values\"].apply(pd.Series)\n",
    "\n",
    "# Merge expanded p-values back into the main DataFrame and drop the original dictionary column\n",
    "df_model_results4 = pd.concat([df_model_results.drop(columns=[\"p_values\"]), df_pvalues], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3.000000\n",
       "mean     0.922295\n",
       "std      0.006529\n",
       "min      0.915572\n",
       "25%      0.919136\n",
       "50%      0.922701\n",
       "75%      0.925656\n",
       "max      0.928612\n",
       "Name: corre_forecasted, dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model_results4.corre_forecasted.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3.000000\n",
       "mean     0.202850\n",
       "std      0.042649\n",
       "min      0.154394\n",
       "25%      0.186929\n",
       "50%      0.219463\n",
       "75%      0.227078\n",
       "max      0.234693\n",
       "Name: pseudo_R2, dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model_results4.pseudo_R2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_model_results4[(df_model_results4.phc_4_lag_0 <= 0.05)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>co_imed</th>\n",
       "      <th>pseudo_R2</th>\n",
       "      <th>conf_int</th>\n",
       "      <th>log_likelihood</th>\n",
       "      <th>deviance</th>\n",
       "      <th>pearson_chi2</th>\n",
       "      <th>corre_forecasted</th>\n",
       "      <th>p_value_corr_forecast</th>\n",
       "      <th>Intercept</th>\n",
       "      <th>time_trend</th>\n",
       "      <th>Reconstructed</th>\n",
       "      <th>aih_4_lag_1</th>\n",
       "      <th>phc_4_lag_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>170009</td>\n",
       "      <td>0.219463</td>\n",
       "      <td>[[-1.074425704742235, 1.0387555001696311], [-0...</td>\n",
       "      <td>-150.574984</td>\n",
       "      <td>6.221183</td>\n",
       "      <td>4.498652</td>\n",
       "      <td>0.928612</td>\n",
       "      <td>1.407848e-13</td>\n",
       "      <td>0.973608</td>\n",
       "      <td>0.293805</td>\n",
       "      <td>0.649750</td>\n",
       "      <td>0.118323</td>\n",
       "      <td>0.092807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>250011</td>\n",
       "      <td>0.234693</td>\n",
       "      <td>[[-0.12941588835031725, 1.5784392648797514], [...</td>\n",
       "      <td>-193.601887</td>\n",
       "      <td>8.594457</td>\n",
       "      <td>4.087071</td>\n",
       "      <td>0.915572</td>\n",
       "      <td>1.358358e-12</td>\n",
       "      <td>0.096328</td>\n",
       "      <td>0.345057</td>\n",
       "      <td>0.977644</td>\n",
       "      <td>0.493401</td>\n",
       "      <td>0.122984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>310054</td>\n",
       "      <td>0.154394</td>\n",
       "      <td>[[0.40164719425602535, 2.457195696654435], [-0...</td>\n",
       "      <td>-196.074136</td>\n",
       "      <td>10.704596</td>\n",
       "      <td>6.917050</td>\n",
       "      <td>0.922701</td>\n",
       "      <td>4.131869e-13</td>\n",
       "      <td>0.006413</td>\n",
       "      <td>0.583266</td>\n",
       "      <td>0.319958</td>\n",
       "      <td>0.163316</td>\n",
       "      <td>0.102974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   co_imed  pseudo_R2                                           conf_int  \\\n",
       "0   170009   0.219463  [[-1.074425704742235, 1.0387555001696311], [-0...   \n",
       "1   250011   0.234693  [[-0.12941588835031725, 1.5784392648797514], [...   \n",
       "2   310054   0.154394  [[0.40164719425602535, 2.457195696654435], [-0...   \n",
       "\n",
       "   log_likelihood   deviance  pearson_chi2  corre_forecasted  \\\n",
       "0     -150.574984   6.221183      4.498652          0.928612   \n",
       "1     -193.601887   8.594457      4.087071          0.915572   \n",
       "2     -196.074136  10.704596      6.917050          0.922701   \n",
       "\n",
       "   p_value_corr_forecast  Intercept  time_trend  Reconstructed  aih_4_lag_1  \\\n",
       "0           1.407848e-13   0.973608    0.293805       0.649750     0.118323   \n",
       "1           1.358358e-12   0.096328    0.345057       0.977644     0.493401   \n",
       "2           4.131869e-13   0.006413    0.583266       0.319958     0.163316   \n",
       "\n",
       "   phc_4_lag_0  \n",
       "0     0.092807  \n",
       "1     0.122984  \n",
       "2     0.102974  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model_results4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process and save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lst_dfs_cities1) + len(lst_dfs_cities2) + len(lst_dfs_cities3) + len(lst_dfs_cities4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['co_imed', 'year_week', 'warning_aih', 'n',\n",
       "       'warning_aih_without_isolated', 'warning_aih_corect_with_consec',\n",
       "       'warning_final_aih', 'aih_4', 'aih_4_lag_1', 'aih_4_lag_2',\n",
       "       'aih_4_lag_3', 'time_trend', 'coef_negbi_aih_4', 'std_err_negbi_aih_4',\n",
       "       'z_negbi_aih_4', 'p_values_negbi_aih_4', 'IC_low_negbi_aih_4',\n",
       "       'IC_high_negbi_aih_4', 'trend_line_negbi_aih_4', 'dtrend_aih_negbi',\n",
       "       'p_value_aih_negbi_friedman', 'test_stat_aih_negbi_friedman',\n",
       "       'year_week_ts', 'epidemi_cal_start', 'epidemi_cal_end', 'atend_ivas',\n",
       "       'num_otc_ivas', 'atend_ivas_4', 'num_otc_ivas_4', 'phc_4_lag_1',\n",
       "       'phc_4_lag_2', 'phc_4_lag_3', 'otc_4_lag_1', 'otc_4_lag_2',\n",
       "       'otc_4_lag_3', 'phc_4_lag_0', 'otc_4_lag_0', 'aih_4_lag_0',\n",
       "       'best_formula', 'best_aic', 'forecast_aih_4', 'fitted_values_aih_aps',\n",
       "       'residuals_aih_aps'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_dfs_cities1[0].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = ['co_imed', 'year_week', 'warning_aih', 'n',\n",
    "       'warning_aih_without_isolated', 'warning_aih_corect_with_consec',\n",
    "       'warning_final_aih', 'aih_4', 'coef_negbi_aih_4', 'std_err_negbi_aih_4',\n",
    "       'z_negbi_aih_4', 'p_values_negbi_aih_4', 'IC_low_negbi_aih_4',\n",
    "       'IC_high_negbi_aih_4', 'trend_line_negbi_aih_4', 'dtrend_aih_negbi',\n",
    "       'p_value_aih_negbi_friedman', 'test_stat_aih_negbi_friedman',\n",
    "       'year_week_ts', 'epidemi_cal_start', 'epidemi_cal_end', 'atend_ivas',\n",
    "       'num_otc_ivas', 'atend_ivas_4', 'num_otc_ivas_4', \n",
    "       'best_formula', 'best_aic', 'forecast_aih_4', 'fitted_values_aih_aps',\n",
    "       'residuals_aih_aps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst1 = []\n",
    "\n",
    "for data in lst_dfs_cities1:\n",
    "\n",
    "    data = data[lst]\n",
    "\n",
    "    lst1.append(data)\n",
    "\n",
    "\n",
    "for data in lst_dfs_cities2:\n",
    "\n",
    "    data = data[lst]\n",
    "\n",
    "    lst1.append(data)\n",
    "\n",
    "for data in lst_dfs_cities3:\n",
    "\n",
    "    data = data[lst]\n",
    "\n",
    "    lst1.append(data)\n",
    "\n",
    "for data in lst_dfs_cities4:\n",
    "\n",
    "    data = data[lst]\n",
    "\n",
    "    lst1.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.concat(lst1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['co_imed', 'year_week', 'warning_aih', 'n',\n",
       "       'warning_aih_without_isolated', 'warning_aih_corect_with_consec',\n",
       "       'warning_final_aih', 'aih_4', 'coef_negbi_aih_4', 'std_err_negbi_aih_4',\n",
       "       'z_negbi_aih_4', 'p_values_negbi_aih_4', 'IC_low_negbi_aih_4',\n",
       "       'IC_high_negbi_aih_4', 'trend_line_negbi_aih_4', 'dtrend_aih_negbi',\n",
       "       'p_value_aih_negbi_friedman', 'test_stat_aih_negbi_friedman',\n",
       "       'year_week_ts', 'epidemi_cal_start', 'epidemi_cal_end', 'atend_ivas',\n",
       "       'num_otc_ivas', 'atend_ivas_4', 'num_otc_ivas_4', 'best_formula',\n",
       "       'best_aic', 'forecast_aih_4', 'fitted_values_aih_aps',\n",
       "       'residuals_aih_aps'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_parquet('/Users/julianeoliveira/Documents/github/Bivariate_Anomaly_Detection_Primary_Health_Care_Drug_Selling_ILI_surveillance/Results/data_manuscript_predict_aih_with_aps_imed.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_results = pd.concat([df_model_results1, df_model_results2, df_model_results3, df_model_results4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_results.to_parquet('/Users/julianeoliveira/Documents/github/Bivariate_Anomaly_Detection_Primary_Health_Care_Drug_Selling_ILI_surveillance/Results/data_manuscript_model_results_predict_aih_with_aps_imed.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
